\documentclass[titlepage]{article}
\usepackage{amsmath, amssymb}
\usepackage{minted}
\newmintinline[cinline]{c}{}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\I}{\mathbb{I}}
\renewcommand{\S}{\texttt{S}}
\newcommand{\xuniform}{\ensuremath{X_{\texttt{uniform}}}}
\newcommand{\zo}{\{0, 1\}}
\newcommand{\uniformbool}{\texttt{uniformbool}}
\newcommand{\p}{\mathbf{p}}
\newcommand{\q}{\mathbf{q}}
\newcommand{\m}{\mathbf{m}}
\renewcommand{\P}{\mathfrak{P}}
\newcommand{\Prop}{\texttt{Prop}}
\newcommand{\M}{\mathfrak{M}}
\newcommand{\K}{\mathfrak{K}}
\newcommand{\qed}{\ensuremath{\triangle}}

\newtheorem{theorem}{Theorem}
\newtheorem{proof}{Proof}[theorem]
\title{A detailed reference of MCMC algorithms}
\author{Siddharth Bhat (20161105) ~ \texttt{siddu.druid@gmail.com}}
\date{\today}
\begin{document}
\maketitle
\section{Why do we need MCMC? A practitioner's perspective}
Consider that we are plonked down in the \texttt{C } programming language, and
our only method to generate random numbers is to call \cinline{int rand(void)}.
However, \emph{the type is a lie}, since we are able to modify global mutable
state. So, really, to have a mathematical discussion about the whole state
of affairs, we will write the type of \cinline{rand} as
$\cinline{rand}: S \rightarrow S \times \cinline{int}$ --- that is,
it receives the entire state of the program, and then returns the \cinline{int},
along with the new state of the program.
$\uniformbool: \S \rightarrow \S \times \zo$.

When we say that \texttt{rand} generates random numbers, we need to be a
little more specific: what does it mean to generate random numbers? we need to
describe the \emph{distribution} according to which we are receiving the random
numbers from the random number generator $rand$.  What does that mean?
Well, it means that as we generate more numbers, the \emph{empirical distribution}
of the list of numbers we get from the successive calls to $\uniformbool$ tends
to some \emph{true distribution}. We will call this \emph{true distribution}
succincty as \textbf{the} distribution of the random number generator. Formally,
let us define $F(t) \equiv \int_0^t P(x) dx$ to be the cumulative distribution 
of $P$.

In the case of 
$\uniformbool$, we are receiving random numbers according to the distribution:
$$
P_{\uniformbool}: \zo \rightarrow [0, 1]; \qquad P_{\uniformbool}(x) = 1/2
$$
That is, both $0$ and $1$ are \emph{equally likely}. However, this is
extremely boring. What we are \emph{usually} interested in is to sample $\{0, 1\}$
in some \emph{biased} fashion:
$$
P_{\uniformbool}^{bias}: \zo \rightarrow [0, 1]; 
\qquad P_{\uniformbool}^{bias}(0) = bias;
\qquad P_{\uniformbool}^{bias}(1) = 1 - bias
$$

And far more generally, we want to sample from \emph{arbitrary domains} with
\emph{arbitrary distributions}:

$$
\texttt{sampler}_X^P: S \rightarrow S \times X; 
$$

This function is boring. What we \emph{really} want to sample from are
more interesting distributions. For example:

\begin{itemize}
    \item The normal distribution $P(x: \R) = e^{-x^2}$.
    \item The poisson distribution $P(x: \N) = e^{-\lambda} \lambda^n/n!$.
    \item A custom distribution $P(x: \R) = |sin(x)|$.
\end{itemize}

\subsection{Fundamental Problem of MCMC sampling}
Given a weak, simple sampler of the form $\cinline{rand} : S \rightarrow S \times \cinline{int}$,
build a sampler $\texttt{sampler}(P, X): T \rightarrow T \times X$ which returns
value distributed according to some \emph{un-normalized}
distribution of choice $P: X \rightarrow \mathbb R$.

The un-normalized constraint is important: it is what allows
us to exploit MCMC to a wide variety of scienarios.

\subsection{Sampling use case 1. Simulation And Sampling}
Often, we do really want samples from a particular distribution. For example,
we might often want to apply Bayes' rule and sample from the posterior
distribution. Recall the formla:

$$
P(Y|X) = P(X|Y).P(Y)/P(X)
$$


\subsection{Sampling use case 2. Gradient Free Optimisation}
We  want to maximize a function $f: X \rightarrow \R$. However, we lack
gradients for $f$, hence we cannot use techniques such as gradient
descent, or other techniques from convex optimisation.

In such a case, we can consider $f$ as some sort of unnormalized
probability distribution, and use MCMC to sample from $f$.


\subsection{Sampling use case 3. Numerical Integration}

\section{Where it all begins: The Metropolis Hastings sampler}
\subsection{The big idea}
We wish to sample from a distribution $\P$, but we do not know how to do so.
The idea is that we build a markov chain $M[\P, \Prop]$ where $\P, \Prop$ are
supplied by the user. We will show that the \emph{stationary distribution} of $M[\P, \Prop]$
is going to be $\P$. This will ensure that if we interpret \emph{states of $M$} as samples,
these samples will the distriubted according to $\P$.

\subsection{Detail balance: A tool for proofs}
We will first require a condition that will enable to rapidly establish that some
distribution $\P: X \rightarrow \R$ is the stationary distribution of a markov chain $M$. If
the transition kernel $K: X \times X \rightarrow \R$ of $M \equiv (X, K)$ is such that:

$$
\forall x, x' \in X, \P(x) K(x, x') = \P(x') K(x', x)
$$

Then $\P$ is said to be \textbf{detail balanced} with respect to $M$.

\begin{theorem}
If $\P$ is detail balanced to $M \equiv (X, K: X \rightarrow (X \rightarrow [0, 1])$,
then $\P$ is the stationary distribution of $M$.
\end{theorem}
\begin{proof}
Let $\P$ be detail balanced to $M$. This means that:
$$
\forall x, x' \in X, \P(x) K(x)(x') = \P(x') K(x')(x)
$$

Let us say say that we are in state $\P$. We wish the find the probability
distribution after one step of transition. The probability of being
in some state $x'_0$ is going to be:

$$
Pnext(x'_0) \equiv \sum_x \P(x) K(x)(x'_0)
$$

since we have $\P(x)$ probability to be at a given $x$, and
$K(x, x'_0)$  probability to go from $x$ to $x'_0$. If we add over all possible
$x \in X$, we get the probability of all states to enter in $x'_0$.
Manipulating $Pnext$, we get:

\begin{align*}
&Pnext(x'_0) \equiv \sum_x \P(x) K(x)(x'_0) \\
&= \sum_x \P(x'_0) K(x'_0)(x) \quad \text{(by detail balance)} \\
&= \P(x'_0) \sum_x K(x'_0)(x) \quad \text{($P(x_0)$ is constant} \\
&= \P(x'_0) \cdot 1 \quad \text{($K(x'_0)$ is a distribution which is being summed over)} \\
&= \P(x'_0) \quad \text{(eliminate multiplication with 1)}
\end{align*}

Hence, $Pnext(n'_0) = \P(x'_0)$ if $\P$ is the current state, and $\P$
is in detail balance with the kernel $K$.
This means that $\P$ is the stationary distribution of $M$.
\qed
\end{proof}

\subsection{Metropolis Hastings}

There are three key players in the metropolis hasting sampler:
\begin{itemize}
    \item [1] $\P: X \rightarrow \R$: the probability distribution we wish to sample from.
    \item[2] $\Prop: X \rightarrow (X \rightarrow \R)$. For each $x_0 \in X$,
        provide a distribution $\Prop(x): X \rightarrow \R$ that is used to
        sample points around $x_0$.  $\Prop$ for \emph{proposal}.
    \item [3] $M[\P, \Prop] \equiv (X, K[\P, \Prop]: X \rightarrow \R)$: The
        Metropolis Markov chain we will sample from, whose stationary distribution is $\P$ ---
        $M$ for \emph{Markov}.
\end{itemize}

We want the stationary distribution of $M[\P, \Prop]$ to be $\P$. We also
wish for $K[\P, \Prop](x_0) \sim \Prop(x_0)$: That is, at a point $x_0$, we want
to choose new points in a way that is 'controlled' by the proposal distribution
$\Prop(x_0)$, since this will allow us to 'guide' the markov chain towards regions where $\P$ is high.
If we had a gradient, then we could use $\P'$ to 'move' from the current point
$x_0$ to a new point. Since we lack a gradient, we will provide a custom
$\Prop(x_0)$ for each $x_0$ that will tell us how to pick a new $x'$, in
a way that will improve $\P$.  So, we tentatively define
$$K[\P, \Prop](x)(x') \stackrel{?}{\equiv} \Prop(x)(x').$$

Recall that for $\P$ to be a stationary distribution of $K$, it is sufficient
for $\P$ to be in detail balance for $K$. So, we write:

\begin{align*}
&\P(x) K(x, x') \stackrel{?}{=}  \P(x') K(x', x) \quad \text{(going forward equally likely as coming back)} \\
&\P(x) Prop(x)(x') \stackrel{?}{=} \P(x') Prop(x')(x) \quad \text{(this is a hard condition to satisfy)}
\end{align*}

This is far too complicated a condition to impose on $Prop$ and $\P$, and there
is no reason for this condition to be satisfied in general. Hence,
we add a custom "fudge factor" $\alpha \in X \rightarrow (X \rightarrow \R)$
that tells us how often to transition
from $x$ to $x'$. We redefine the kernel as:

$$
K[\P, \Prop](x)(x') \equiv \Prop(x)(x') \alpha(x)(x')
$$

Redoing detail balance with this new $K$, we get:

\begin{align*}
&\P(x) K(x, x') \stackrel{?}{=}  \P(x') K(x', x) \quad \text{(going forward equally likely as coming back)} \\
&\P(x) Prop(x)(x') \alpha(x)(x') = \P(x') Prop(x')(x) \alpha(x')(x) \quad \text{(Fudge a hard condition with $\alpha$)} \\
&\frac{\alpha(x)(x')}{\alpha(x')(x)} = \frac{\P(x') Prop(x)(x') }{\P(x) Prop(x')(x)} \quad \text{(Find conditions for $\alpha$)}
\end{align*}

What we have above is a \emph{constraint} for $\alpha$. We now need to \emph{pick}
an $\alpha$ that satisfies this. A reasonable choice is:

$$ \alpha(x)(x') \equiv   \min\left(1, \frac{\P(x')\Prop(x')(x)}{\P(x)\Prop(x)(x')} \right) $$.

since $K$ is a transition kernel, we canot have its entries be greater tha n $1$.
Hence, we choose to clamp it with a $\min(1, \cdot)$. This finally gives us the kernel as:
\begin{align*}
&K[\P, \Prop](x)(x') \equiv \Prop(x)(x') \alpha(x)(x') \\
&K[\P, \Prop](x)(x') = \Prop(x)(x') \min\left(1, \frac{\P(x')\Prop(x')(x)}{\P(x)\Prop(x)(x')} \right) \\
&K[\P, \Prop](x)(x') =   \min\left(\Prop(x)(x'), \frac{\P(x')\Prop(x')(x)}{\P(x)} \right) \\
\end{align*}

We can make sure that detail balance is satisfied:

\begin{align*}
&\P(x) K[\P, \Prop](x)(x') \stackrel{?}{=} \P(x')K[\P, \Prop](x')(x) \\
%
&\P(x)  \min\left(\Prop(x)(x'), \frac{\P(x')\Prop(x')(x)}{\P(x)} \right) \stackrel{?}{=}
\P(x')  \min\left(\Prop(x')(x), \frac{\P(x)\Prop(x)(x')}{\P(x')} \right) \\
%
&\min\left(\P(x)\Prop(x)(x'), \P(x) \frac{\P(x')\Prop(x')(x)}{\P(x)} \right) \stackrel{?}{=}
\min\left(\P(x') \Prop(x')(x), \P(x') \frac{\P(x)\Prop(x)(x')}{\P(x')} \right) \\
%
&\min\left(\P(x)\Prop(x)(x'), \P(x')\Prop(x')(x) \right) {=}
\min\left(\P(x')\Prop(x')(x), \P(x)\Prop(x)(x') \right)
\end{align*}


\section{Gibbs sampling}
We wish to sample from a joint distribution $P(X_1, X_2, X_3)$. However, it might
be far cheaper to sample $P(X_1 | X_2, X_3)$, $P(X_2 | X_1, X_3)$, and
$P(X_3 | X_1, X_2)$. If it is indeed cheaper, then we can use a Gibbs sampler
to draw from the actual distribution $P(X_1, X_2, X_3)$ by combining samples
from the \emph{conditional} distribution cleverly. The code is:

\begin{minted}{py}
# sampler_x: y, z -> new x
# sampler_y: x, z -> new y
# sampler_z: x, y -> new z
# N: number of iterations.
# returns: new (x, y, z)
def sampler_xyz(sampler_x, sampler_y, sampler_z, x0, y0, z0, N):
    (x, y, z) = (x0, y0, z0)
    for i in range(N):
        x = sampler_x(y, z)
        y = sampler_y(x, z) # NOTE: use *new* x
        z = sampler_z(x, y) # NOTE: use *new* x, y
    return (x, y, z)
\end{minted}

\subsection{Gibbs sampling maintains detail balance}
% https://stats.stackexchange.com/questions/118442/does-the-gibbs-sampling-algorithm-guarantee-detailed-balance
Consider a two-variate state where we sample the first variable from its conditional distribution.
A move between  $(x_1, x_2)$ and $(y_1, y_2)$ has zero probability in both directions
if $x_2\neq y_2$, and thus detail balance automatically holds. If $x_2 = y_2$, then
we can consider:

\begin{align*}
&\pi(x_1, x_2)  Prob((x_1, x_2) \rightarrow (y_1, x_2)) \\
&= \pi(x_1, x_2) p(y_1 | X_2 = x_2) \\
&= \pi(x_1, x_2) \frac{\pi(y_1, x_2)}{\sum_z \pi(z, x_2)} \\
&= \pi(y_1, x_2) \frac{\pi(x_1, x_2)}{\sum_z \pi(z, x_2)} \quad \text{[move $(y_1, x_2)$ from the fraction outside]} \\
&=\pi(y_1, x_2) p(x_1 | X_2 = x_2) \\
&= \pi(y_1, x_2) Prob((y_1, x_2) \rightarrow (x_1, x_2))
\end{align*}

\section{Hamiltonian Monte Carlo}
% https://www.cs.toronto.edu/~radford/ftp/ham-mcmc.pdf
% http://www.mcmchandbook.net/HandbookChapter5.pdf
% https://arxiv.org/pdf/1701.02434.pdf

If our target probability density $P: X \rightarrow [0, 1]$ is \emph{differentiable},
then we can use the derivative of $P(x)$ to provide better proposals. The idea
is as follows:

\begin{itemize}
    \item Interpret the probability landscape as actual terrain, with
        points of high probability being "valleys" and points of low probability
        being "peaks" (ie, invert the probability density with a transform
        such as $terrian(x) = e^{-P(x)}$
    \item For a proposal at a position $x_0 \in X$, keep a ball at $x_0$,
        \emph{randomly choose its velocity}, simulate the ball according to 
        classical mechanics (Newton's laws of motion) for a fixed duration $D \in \R$
        and propose the final position as the final position of the ball. This
        is reversible and detail balanced because 
        \emph{classical mechanics is reversible and detail balanced}.
\end{itemize}
We need the derivative of $P(x)$ to be able to simulate the ball according
to Newton's laws. If we can do this, though, we are able to cover large
amounts of terrain.


\subsection{Newton's laws of motion: Hamilton's equations}

\begin{align*}
    \frac{\partial \q}{\partial t} = \frac{\partial H}{\partial \p} \qquad
    \frac{\partial \p}{\partial t} = - \frac{\partial H}{\partial \q}
\end{align*}

In our case, the choice of the hamiltonian will consider the negative-log-probability-density
to be the \emph{energy}. So, higher probability has lower energy. We know that
particles like to go towards lower energy states.

$$
H(\q, \p) \equiv  U(\q) + K(\p) \quad K(\p) \equiv p^T M^{-1} p/2
$$

where $M$ is a symmetric positive-definite matrix known as a ``mass matrix''.


\subsection{Reversibility}

\subsection{Detail balance}

We wish to show that:

$\pi(\p, \q) P((\p, \q) \rightarrow (\p', \q')) = \pi(\p', \q') P((\p', \q') \rightarrow (\p, \q))$


\subsection{Hamiltonian for our simulation}
If we have a a target probability distribution $Prob(\q): \R^n \rightarrow \R$
we choose the Hamiltonian to be $H(\p, \q) \equiv Prob(\q) + \p^T \p/2$.


\subsection{Simulation: Naive}
\subsection{Simulation: Need for symplectic integrators}
\subsection{Simulation: Final}


We no longer need to choose how many steps to walk with a no-U-turn-sampler.
It prevents us from re-walking energy orbits, by detecting when we have completed
traversing an orbit and are going to take a "U-turn". The details are quite
complex, so we may not cover this here.

\section{Discontinuous Hamiltonian monte carlo}

What if our distribution $P$ is \emph{discontinuouse}? How do we perform MCMC
in that case?

\subsection{Hamilton's equations under laplace momntum}

\begin{align*}
    \frac{d \q}{dt} = \m^{-1} \odot sign(\p) \qquad \frac{d \p}{dt} = - \nabla_q V(q)
\end{align*}
where $\odot$ is component-wise multiplication. If we know that $\p$ does
not change sign, then our dynamics are correct; This is very different from
the usual hamilton's equations, where we need to know the \emph{magnitude}
of $\p$.

So, as long as we know that $\p$ has not changed sign, we can hold $sign(\p)$
consant and use:

$$
q(t + \epsilon) = q(t) + \epsilon m^{-1} \odot sign(\p(t))
$$

Thus, we can jump across multiple discontinuities is $V(q)$ as long as we are
aware that $sign(p(t))$ does not change.

TODO: write about the sudden drop in U when we cross a barrier.

\section{Low discrepancy sequences}
% https://cseweb.ucsd.edu/~dstefan/pubs/dalal:2008:low.pdf
% https://openturns.github.io/openturns/master/theory/reliability_sensitivity/low_discrepancy_sequence.html
Low discrepancy sequences are sequences of numbers that more evenly
distributed than pseudorandom numbers in high dimensional space. Hence,
simulations which use Low discrepancy sequences generally approximate
integrals faster than psuedo-randomly generated points.

Formally, let us consider the $S$ dimensional half-open cube $\I^S \equiv [0, 1)^S$.
assume we have a set of points $P \subseteq \I^S$, and a sub-interval
$B \subseteq I^S$, where a sub-interval is a subset of the form 
$B \equiv \prod_{i=1}^S \{ x \in \I : a_i \leq x \leq b_i \}$.

Given a universe set $X$ and two subsets $Large, Small \subseteq X$, we define
the amount of containment of $Small$ in $Large$ to be $C(Small, Large) \equiv
\frac{|Large \cap Small|}{|Large|}$. Intuitively, this measures the fraction
of $Small$ that is in $Large$. We now define the discrepancy of the set
of points $P$ relative to the sub-interval $B$ as:

\begin{align*}
&D(B, P) \equiv \left| C(P, B) - C(B, \I^S) \right| \\
&= \left| \frac{|B \cap P|}{|P|} - \frac{|B \cap \I^S|}{|\I^S|} \right|  \\
&= \left| \frac{|B \cap P|}{|P|} - \frac{Volume(B)}{1} \right| \\
&= \left| \frac{|B \cap P|}{|P|} - Volume(B) \right| \\
\end{align*}

So, the discrepancy is measuring if $P$ fits within $B$ the way $B$ fits within
the full space.

Now, the \textbf{worst-case-discrepancy} is defined as the maximum discrepancy
over all sub-intervals:

$$
D^\star(P) \equiv \max_{B \in \mathcal{J}} D(B, P)
$$

where $\mathcal{J} \subseteq 2^{\I^S}$ is the set of all sub-intervals:

$$
\mathcal{J} \equiv \{ \{ x \in \I^S : l[i] \leq x[i] \leq r[i]~\forall i \} : \vec l, \vec r \in \I^S \}
$$

The goal is a \emph{low discrepancy sequence} is to minimise the worst-case-discrepancy.

\subsection{Error bounds for Numerical integration}

\subsection{Sobol sequences}

Sobol sequences are an example of low-discrepancy sequences


\section{Future Work!}
I've left the topic names up because they have interesting ideas, which I have
sketched out in brief. Writing them down formally will take far too long; Hence,
they've been left here [for the purposes of the project]. I will probably update
this document even after the project, once I have grokked this material better.
\section{Slice Sampling}

\section{No-U-Turn sampling}

\end{document}

